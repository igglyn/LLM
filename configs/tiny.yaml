tokenizer:
  patch_size: 1
  add_bos: true
  add_eos: true

data:
  raw_path: data/raw
  pattern: "*.txt"
  processed_dir: data/processed
  train_split: 0.95
  seq_len: 256

model:
  d_model: 384
  n_layers: 6
  n_heads: 6
  dropout: 0.1

patcher:
  latent_dim: 384
  encoder_layers: 2
  decoder_layers: 2
  n_heads: 6
  dropout: 0.1
  pretrained_path: ""
  freeze: false

patcher_train:
  batch_size: 8
  max_steps: 3000
  eval_every: 100
  eval_batches: 50
  save_every: 200
  lr: 0.0003
  weight_decay: 0.01
  grad_clip: 1.0
  out_dir: outputs/patcher

train:
  seed: 42
  batch_size: 8
  grad_accum_steps: 4
  max_steps: 15000
  eval_every: 100
  eval_batches: 50
  save_every: 200
  lr_max: 0.0003
  lr_min: 0.00003
  warmup_steps: 1000
  weight_decay: 0.1
  grad_clip: 1.0
  out_dir: outputs

sample:
  temperature: 0.9
  top_k: 40
  max_new_tokens: 128
