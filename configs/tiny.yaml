tokenizer:
  patch_size: 8
  vocab_size: 4096
  min_freq: 1
  add_bos: true
  add_eos: true

data:
  raw_path: data/raw
  pattern: "*.txt"
  processed_dir: data/processed
  train_split: 0.95
  seq_len: 256

model:
  d_model: 384
  n_layers: 6
  n_heads: 6
  dropout: 0.1

train:
  seed: 42
  batch_size: 8
  grad_accum_steps: 4
  max_steps: 15000
  eval_every: 100
  save_every: 200
  lr_max: 0.0003
  lr_min: 0.00003
  warmup_steps: 1000
  weight_decay: 0.1
  grad_clip: 1.0
  out_dir: outputs

sample:
  temperature: 0.9
  top_k: 40
  max_new_tokens: 128
